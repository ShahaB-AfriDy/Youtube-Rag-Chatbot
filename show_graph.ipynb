{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abb754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from psycopg import Connection\n",
    "from langgraph.graph import Graph, END\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ----------------- SETUP -----------------\n",
    "load_dotenv()\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5432/rag?sslmode=disable\"\n",
    "embedding_dim = 1536  # Use reduced dimension\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    embedding_kwargs={\"output_dimensionality\": embedding_dim},\n",
    ")\n",
    "\n",
    "conn = Connection.connect(DB_URI, autocommit=True)\n",
    "store = PostgresStore(conn, index={\"embed\": embeddings, \"dims\": embedding_dim, \"hnsw\": False})\n",
    "store.setup()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# ----------------- GRAPH LOGIC -----------------\n",
    "namespace = (\"1\", \"documents\")\n",
    "\n",
    "# --- Node: Start ---\n",
    "def start_node(state):\n",
    "    mode = input(\"Choose Mode: (1) Ingest Text/URL  (2) Chat : \").strip()\n",
    "    state[\"mode\"] = mode\n",
    "    return \"mode_selector\"\n",
    "\n",
    "# --- Node: Mode Selector ---\n",
    "def mode_selector(state):\n",
    "    if state[\"mode\"] == \"1\":\n",
    "        state[\"input_source\"] = input(\"Enter story text or YouTube URL: \").strip()\n",
    "        return \"check_exists\"\n",
    "    elif state[\"mode\"] == \"2\":\n",
    "        return \"chat_query\"\n",
    "    else:\n",
    "        print(\"‚ùå Invalid mode selected.\")\n",
    "        return END\n",
    "\n",
    "# --- Node: Check if URL/Text Exists ---\n",
    "def check_exists(state):\n",
    "    user_input = state[\"input_source\"]\n",
    "    results = store.search(namespace, query=user_input, limit=1)\n",
    "    if results:\n",
    "        print(\"‚úÖ Already embedded, switching to chat mode...\")\n",
    "        return \"chat_query\"\n",
    "    else:\n",
    "        return \"extract_content\"\n",
    "\n",
    "# --- Node: Extract or Transcribe Content ---\n",
    "def extract_content(state):\n",
    "    user_input = state[\"input_source\"]\n",
    "    if user_input.startswith(\"http\"):\n",
    "        # Placeholder ‚Äî You can add yt_dlp + Whisper transcription here\n",
    "        text = \"This is a transcribed text from the given URL.\"\n",
    "    else:\n",
    "        text = user_input\n",
    "    state[\"text\"] = text\n",
    "    return \"split_chunks\"\n",
    "\n",
    "# --- Node: Split Text ---\n",
    "def split_chunks(state):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    state[\"chunks\"] = splitter.split_text(state[\"text\"])\n",
    "    return \"embed_chunks\"\n",
    "\n",
    "# --- Node: Embed and Store ---\n",
    "def embed_chunks(state):\n",
    "    chunks = state[\"chunks\"]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        key = f\"chunk_{i+1}\"\n",
    "        metadata = {\"text\": chunk}\n",
    "        store.put(namespace, key, metadata, index=[\"text\"])\n",
    "    print(f\"‚úÖ Stored {len(chunks)} chunks in PostgreSQL.\")\n",
    "    return \"generate_summary\"\n",
    "\n",
    "# --- Node: Generate Summary ---\n",
    "def generate_summary(state):\n",
    "    text = state[\"text\"]\n",
    "    summary = llm.invoke([HumanMessage(content=f\"Summarize this story:\\n\\n{text}\")])\n",
    "    print(\"\\nüìù Summary of the text:\\n\", summary.content)\n",
    "    return \"chat_query\"\n",
    "\n",
    "# --- Node: Chat with User ---\n",
    "def chat_query(state):\n",
    "    while True:\n",
    "        query = input(\"\\nüí¨ Ask a question (or 'exit'): \").strip()\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"üëã Ending chat.\")\n",
    "            return END\n",
    "        results = store.search(namespace, query=query, limit=3)\n",
    "        context = \"\\n\".join([r.value[\"text\"] for r in results])\n",
    "        prompt = f\"Context:\\n{context}\\n\\nUser Question: {query}\"\n",
    "        answer = llm.invoke([HumanMessage(content=prompt)])\n",
    "        print(\"\\nü§ñ Answer:\", answer.content)\n",
    "\n",
    "# ----------------- BUILD GRAPH -----------------\n",
    "graph = Graph()\n",
    "graph.add_node(\"start\", start_node)\n",
    "graph.add_node(\"mode_selector\", mode_selector)\n",
    "graph.add_node(\"check_exists\", check_exists)\n",
    "graph.add_node(\"extract_content\", extract_content)\n",
    "graph.add_node(\"split_chunks\", split_chunks)\n",
    "graph.add_node(\"embed_chunks\", embed_chunks)\n",
    "graph.add_node(\"generate_summary\", generate_summary)\n",
    "graph.add_node(\"chat_query\", chat_query)\n",
    "\n",
    "# --- Edges ---\n",
    "graph.set_entry_point(\"start\")\n",
    "graph.add_edge(\"start\", \"mode_selector\")\n",
    "graph.add_edge(\"mode_selector\", \"check_exists\")\n",
    "graph.add_edge(\"check_exists\", \"extract_content\")\n",
    "graph.add_edge(\"extract_content\", \"split_chunks\")\n",
    "graph.add_edge(\"split_chunks\", \"embed_chunks\")\n",
    "graph.add_edge(\"embed_chunks\", \"generate_summary\")\n",
    "graph.add_edge(\"generate_summary\", \"chat_query\")\n",
    "graph.add_edge(\"mode_selector\", \"chat_query\")\n",
    "\n",
    "app = graph.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1500a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Global2 (Python 3.12.5)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"\\nSession {i+1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Global2)",
   "language": "python",
   "name": "global2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
